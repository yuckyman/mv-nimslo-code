{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nimslo Image Alignment Demo\n",
    "\n",
    "This notebook demonstrates the complete pipeline for aligning Nimslo 4-lens camera images and generating boomerang GIFs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Basic imports successful\n",
      "✓ Core modules loaded (segmentation deferred)\n",
      "✓ All modules loaded successfully!\n",
      "✓ Brightness normalization available (normalize_brightness, brightness_strength)\n",
      "\n",
      "⚠ Note: Segmentation will be loaded on-demand to avoid kernel crashes\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import importlib\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# CRITICAL: Set these BEFORE importing anything that uses OpenMP (rembg/onnxruntime)\n",
    "# This prevents kernel crashes from OMP conflicts and fixes deprecated warnings\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "os.environ['OMP_NUM_THREADS'] = '1'  # Limit threads to avoid conflicts\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "os.environ['OMP_MAX_ACTIVE_LEVELS'] = '1'  # Use max_active_levels instead of deprecated nested\n",
    "\n",
    "# Try to configure OpenMP programmatically\n",
    "try:\n",
    "    import ctypes\n",
    "    try:\n",
    "        # Set max_active_levels directly if possible\n",
    "        libomp = ctypes.CDLL(None)\n",
    "        if hasattr(libomp, 'omp_set_max_active_levels'):\n",
    "            libomp.omp_set_max_active_levels(1)\n",
    "            print(\"✓ OpenMP configured to use max_active_levels\")\n",
    "    except:\n",
    "        pass\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', message='.*omp_set_nested.*')\n",
    "\n",
    "# Add code directory to path\n",
    "code_dir = Path().absolute()\n",
    "if str(code_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(code_dir))\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "print(\"✓ Basic imports successful\")\n",
    "\n",
    "# Import and reload modules to pick up any changes (useful during development)\n",
    "try:\n",
    "    import nimslo_core.gif_generator\n",
    "    importlib.reload(nimslo_core.gif_generator)\n",
    "except Exception as e:\n",
    "    print(f\"Note: Could not reload module (this is OK on first run): {e}\")\n",
    "\n",
    "from nimslo_core import (\n",
    "    preprocess_image,\n",
    "    align_images,\n",
    "    extract_features,\n",
    "    match_features\n",
    ")\n",
    "\n",
    "# Import create_boomerang_gif AFTER reload to get latest version\n",
    "from nimslo_core.gif_generator import create_boomerang_gif\n",
    "\n",
    "# DON'T import segmentation here - it will crash the kernel\n",
    "# We'll import it only when needed, or use depth-based segmentation\n",
    "print(\"✓ Core modules loaded (segmentation deferred)\")\n",
    "\n",
    "# Verify the function has the expected parameters\n",
    "import inspect\n",
    "sig = inspect.signature(create_boomerang_gif)\n",
    "has_normalize = 'normalize_brightness' in sig.parameters\n",
    "has_strength = 'brightness_strength' in sig.parameters\n",
    "\n",
    "print(\"✓ All modules loaded successfully!\")\n",
    "if has_normalize and has_strength:\n",
    "    print(\"✓ Brightness normalization available (normalize_brightness, brightness_strength)\")\n",
    "else:\n",
    "    print(\"⚠ Warning: Brightness normalization parameters not found - restart kernel if needed\")\n",
    "print(\"\\n⚠ Note: Segmentation will be loaded on-demand to avoid kernel crashes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "RAW_DIR = Path(\"../nimslo_raw\")\n",
    "OUTPUT_DIR = Path(\"../outputs\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "BATCH_NAME = \"12\"  # Change this to process different batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Available Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19 batches.\n"
     ]
    }
   ],
   "source": [
    "# Find all batch directories\n",
    "batch_dirs = sorted([\n",
    "    d for d in RAW_DIR.iterdir()\n",
    "    if d.is_dir() and (d.name.isdigit() or d.name.replace(\"-\", \"\").isdigit())\n",
    "])\n",
    "\n",
    "print(f\"Found {len(batch_dirs)} batches.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4 images from 12\n",
      "Original image dimensions: (2137, 1535, 3)\n"
     ]
    }
   ],
   "source": [
    "# Load images from selected batch\n",
    "batch_path = RAW_DIR / BATCH_NAME\n",
    "image_files = sorted(batch_path.glob('*.jpg')) + sorted(batch_path.glob('*.JPG'))\n",
    "image_files = image_files[:4]  # Take first 4 images\n",
    "\n",
    "# Store original images (before any processing)\n",
    "original_images = []\n",
    "for f in image_files:\n",
    "    img = cv2.imread(str(f))\n",
    "    if img is not None:\n",
    "        original_images.append(img)\n",
    "\n",
    "# Normalize sizes of originals (just cropping, no denoising/contrast adjustment)\n",
    "from nimslo_core.preprocessing import normalize_sizes\n",
    "original_images = normalize_sizes(original_images)\n",
    "\n",
    "print(f\"Loaded {len(original_images)} images from {BATCH_NAME}\")\n",
    "print(f\"Original image dimensions: {original_images[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed 4 images for alignment\n",
      "Preprocessed image dimensions: (2137, 1535, 3)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Preprocess images for feature detection/alignment (denoise and contrast adjust)\n",
    "# Note: We'll use these for alignment, but apply transformations to originals for final GIF\n",
    "preprocessed = [preprocess_image(img, denoise=True) for img in original_images]\n",
    "\n",
    "print(f\"Preprocessed {len(preprocessed)} images for alignment\")\n",
    "print(f\"Preprocessed image dimensions: {preprocessed[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subject Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using depth-based segmentation (rembg crashes Jupyter kernels)...\n",
      "Segmenting subject (this may take a moment on first run)...\n"
     ]
    }
   ],
   "source": [
    "# Segment subjects from each frame\n",
    "# Use depth-based segmentation by default (rembg crashes Jupyter kernels)\n",
    "# Depth-based works reliably and doesn't require rembg/onnxruntime\n",
    "print(\"Using depth-based segmentation (rembg crashes Jupyter kernels)...\")\n",
    "from nimslo_core.segmentation import segment_subject\n",
    "\n",
    "print(\"Segmenting subject (this may take a moment on first run)...\")\n",
    "mask, conf = segment_subject(preprocessed[0], method=\"depth\", return_confidence=True)\n",
    "\n",
    "print(f\"\\nSegmentation result:\")\n",
    "print(f\"  Method: depth (Intel DPT)\")\n",
    "print(f\"  Confidence: {conf:.2f}\")\n",
    "\n",
    "# Note: To use U²-Net (rembg), use the CLI instead:\n",
    "# python nimslo_cli.py ../nimslo_raw/01/ -o test.gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get masks for all frames using depth-based segmentation\n",
    "masks = []\n",
    "for i, img in enumerate(preprocessed):\n",
    "    try:\n",
    "        mask, conf = segment_subject(img, method=\"depth\", return_confidence=True)\n",
    "        masks.append(mask)\n",
    "        print(f\"Frame {i+1}: confidence={conf:.2f}, method=depth\")\n",
    "    except Exception as e:\n",
    "        print(f\"Frame {i+1}: Error - {e}\")\n",
    "        # Create fallback mask (center region)\n",
    "        h, w = img.shape[:2]\n",
    "        mask = np.zeros((h, w), dtype=np.uint8)\n",
    "        y1, y2 = int(h*0.2), int(h*0.8)\n",
    "        x1, x2 = int(w*0.2), int(w*0.8)\n",
    "        mask[y1:y2, x1:x2] = 255\n",
    "        masks.append(mask)\n",
    "        print(f\"Frame {i+1}: Using fallback mask (center region)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize segmentation masks overlaid on images\n",
    "from nimslo_core.segmentation import visualize_mask\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "# Top row: original preprocessed images\n",
    "for i, (ax, img) in enumerate(zip(axes[0], preprocessed)):\n",
    "    ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    ax.set_title(f\"Frame {i+1} (Preprocessed)\")\n",
    "    ax.axis('off')\n",
    "\n",
    "# Bottom row: images with segmentation masks overlaid\n",
    "for i, (ax, img, mask) in enumerate(zip(axes[1], preprocessed, masks)):\n",
    "    overlay = visualize_mask(img, mask, alpha=0.4, color=(0, 255, 0))\n",
    "    ax.imshow(cv2.cvtColor(overlay, cv2.COLOR_BGR2RGB))\n",
    "    ax.set_title(f\"Frame {i+1} (Segmented)\")\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle(\"Subject Segmentation Results\", fontsize=14, y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction and Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from first two frames\n",
    "kp1, des1 = extract_features(preprocessed[0], mask=masks[0], n_features=1000)\n",
    "kp2, des2 = extract_features(preprocessed[1], mask=masks[1], n_features=1000)\n",
    "\n",
    "print(f\"Frame 1: {len(kp1)} keypoints, {des1.shape[0] if des1 is not None else 0} descriptors\")\n",
    "print(f\"Frame 2: {len(kp2)} keypoints, {des2.shape[0] if des2 is not None else 0} descriptors\")\n",
    "\n",
    "# Match features (match_features already applies ratio test)\n",
    "matches = match_features(des1, des2)\n",
    "\n",
    "print(f\"\\nMatches after ratio test: {len(matches)}\")\n",
    "if len(matches) > 0:\n",
    "    distances = [m.distance for m in matches]\n",
    "    print(f\"  Distance range: {min(distances):.2f} - {max(distances):.2f}\")\n",
    "    print(f\"  Mean distance: {np.mean(distances):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize keypoints detected on each image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Frame 1: Show keypoints\n",
    "img1_kp = cv2.drawKeypoints(\n",
    "    preprocessed[0], kp1, None,\n",
    "    flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS\n",
    ")\n",
    "axes[0].imshow(cv2.cvtColor(img1_kp, cv2.COLOR_BGR2RGB))\n",
    "axes[0].set_title(f\"Frame 1: {len(kp1)} keypoints\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Frame 2: Show keypoints\n",
    "img2_kp = cv2.drawKeypoints(\n",
    "    preprocessed[1], kp2, None,\n",
    "    flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS\n",
    ")\n",
    "axes[1].imshow(cv2.cvtColor(img2_kp, cv2.COLOR_BGR2RGB))\n",
    "axes[1].set_title(f\"Frame 2: {len(kp2)} keypoints\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.suptitle(\"Feature Keypoints Detection\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize matches (use matches from match_features, not filtered)\n",
    "if len(matches) > 0:\n",
    "    # Sort by distance to show best matches first\n",
    "    sorted_matches = sorted(matches, key=lambda x: x.distance)\n",
    "    \n",
    "    img_matches = cv2.drawMatches(\n",
    "        preprocessed[0], kp1,\n",
    "        preprocessed[1], kp2,\n",
    "        sorted_matches[:50],  # Show best 50 matches\n",
    "        None,\n",
    "        flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS\n",
    "    )\n",
    "    \n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.imshow(cv2.cvtColor(img_matches, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(f\"Feature Matches Between Frame 1 and Frame 2 (showing best {min(50, len(matches))} of {len(matches)})\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No matches found! This could indicate:\")\n",
    "    print(\"  - Masks are too restrictive (not enough overlap)\")\n",
    "    print(\"  - Images are too different\")\n",
    "    print(\"  - Feature extraction failed\")\n",
    "    \n",
    "    # Try without masks to see if that helps\n",
    "    print(\"\\nTrying without masks...\")\n",
    "    kp1_no_mask, des1_no_mask = extract_features(preprocessed[0], mask=None, n_features=1000)\n",
    "    kp2_no_mask, des2_no_mask = extract_features(preprocessed[1], mask=None, n_features=1000)\n",
    "    matches_no_mask = match_features(des1_no_mask, des2_no_mask)\n",
    "    print(f\"Without masks: {len(matches_no_mask)} matches\")\n",
    "    \n",
    "    if len(matches_no_mask) > 0:\n",
    "        sorted_matches = sorted(matches_no_mask, key=lambda x: x.distance)\n",
    "        img_matches = cv2.drawMatches(\n",
    "            preprocessed[0], kp1_no_mask,\n",
    "            preprocessed[1], kp2_no_mask,\n",
    "            sorted_matches[:50],\n",
    "            None,\n",
    "            flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS\n",
    "        )\n",
    "        plt.figure(figsize=(16, 8))\n",
    "        plt.imshow(cv2.cvtColor(img_matches, cv2.COLOR_BGR2RGB))\n",
    "        plt.title(f\"Matches WITHOUT masks (showing best {min(50, len(matches_no_mask))} of {len(matches_no_mask)})\")\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align preprocessed images (for good feature matching)\n",
    "# This gives us the transformation matrices\n",
    "aligned_preprocessed, alignment_results = align_images(\n",
    "    preprocessed, masks,\n",
    "    n_features=1000\n",
    ")\n",
    "\n",
    "print(\"Alignment results:\")\n",
    "for i, result in enumerate(alignment_results):\n",
    "    if i > 0:  # Skip reference frame\n",
    "        print(f\"Frame {i+1}: {result.total_matches} matches, {result.inliers} inliers, IoU: {result.iou:.2f}\")\n",
    "\n",
    "# Apply the same transformations to original images (max quality)\n",
    "print(\"\\nApplying transformations to original images...\")\n",
    "aligned_originals = []\n",
    "ref_h, ref_w = original_images[0].shape[:2]\n",
    "\n",
    "for i, (orig_img, result) in enumerate(zip(original_images, alignment_results)):\n",
    "    if i == 0:\n",
    "        # Reference frame stays as-is\n",
    "        aligned_originals.append(orig_img.copy())\n",
    "    else:\n",
    "        # Apply transformation from alignment result\n",
    "        # transform is stored as 3x3 (affine padded or homography)\n",
    "        transform = result.transform\n",
    "        aligned_originals.append(cv2.warpPerspective(orig_img, transform, (ref_w, ref_h)))\n",
    "\n",
    "print(\"✓ Original images aligned (max quality, no denoising/contrast adjustment)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display aligned original images (max quality)\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "for i, (ax, img) in enumerate(zip(axes, aligned_originals)):\n",
    "    ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    ax.set_title(f\"Aligned Original Frame {i+1}\")\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Boomerang GIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GIF from aligned original images (max quality, no denoising/contrast adjustment)\n",
    "# Optionally resize for web-friendly size (or use full resolution for max quality)\n",
    "from nimslo_core.gif_generator import resize_for_web\n",
    "\n",
    "# For max quality, use full resolution. For web-friendly, uncomment the resize line:\n",
    "aligned_originals = resize_for_web(aligned_originals, max_dimension=600)\n",
    "\n",
    "# Create boomerang GIF from original images\n",
    "# crop_valid_region=True removes black bars from stereoscopic alignment\n",
    "# normalize_brightness=True prevents flashing from exposure differences\n",
    "# brightness_strength controls how much correction (0.0-1.0, default 0.5 = moderate)\n",
    "output_path = OUTPUT_DIR / f\"test_{BATCH_NAME}.gif\"\n",
    "gif_path = create_boomerang_gif(\n",
    "    aligned_originals, \n",
    "    output_path,\n",
    "    crop_valid_region=True,  # Remove black borders from warping\n",
    "    normalize_brightness=True,  # Equalize brightness to prevent flashing\n",
    "    brightness_strength=0.5  # Moderate correction (0.0 = none, 1.0 = full)\n",
    ")\n",
    "\n",
    "print(f\"✓ GIF saved to: {gif_path}\")\n",
    "print(f\"  File size: {gif_path.stat().st_size / 1024:.1f} KB\")\n",
    "print(f\"  Using original images (max quality, no denoising/contrast adjustment)\")\n",
    "print(f\"  Cropped to remove black bars from alignment\")\n",
    "print(f\"  Brightness normalized across frames to prevent flashing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the GIF (if in Jupyter)\n",
    "from IPython.display import Image, display\n",
    "display(Image(str(gif_path)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
